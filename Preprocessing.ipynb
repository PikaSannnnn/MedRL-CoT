{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89d5679-991e-4f17-bcdf-a39beef3ef0e",
   "metadata": {},
   "source": [
    "### Preprocessing for Mimic4 Dataset\n",
    "\n",
    "**Note that the AUG dataset is incomplete, so we skip it here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ad5779-a5ac-4867-9b7f-fc3023047b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e50ca3-9850-42d7-b71f-8390c9a90dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medrlcot.config.env import MedRL_CoT\n",
    "from medrlcot import data_manager\n",
    "from medrlcot.medrlcot_logger import setup_logger\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Features, Value\n",
    "import torch\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets as hf_datasets\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)  \n",
    "# pd.set_option('display.width', 0)    \n",
    "# pd.set_option('display.max_rows', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a0e936-71fc-48ac-b099-6f3050346a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:20:23,500 || INFO || Logger - Setup for MedRL-CoT's log done. This is the beginning of the log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new log file logs/medrlcot039.log\n"
     ]
    }
   ],
   "source": [
    "model_cfg_path = os.path.join(os.getcwd(), \"medrlcot/config/.env\")\n",
    "medrlcot_config = MedRL_CoT(model_cfg_path)\n",
    "\n",
    "setup_logger()\n",
    "logger = logging.getLogger(\"MedRL-CoT Preprocess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c6e504-937a-4abe-9c61-aa5c59490b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aug_med_notes', 'mimic4'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed_dirs = [os.path.join(os.getcwd(), medrlcot_config.data_dir, ds, 'processed') for ds in medrlcot_config.datasets]\n",
    "processed_dirs = {ds: os.path.join(os.getcwd(), medrlcot_config.data_dir, ds, 'processed') for ds in medrlcot_config.datasets}\n",
    "processed_dirs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499d4653-c605-406c-ad24-a1580c7e1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(['symptoms_labs', 'thought_process', 'diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee7871d1-58d9-4dd2-86ed-e4b44df82554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labeled(arrow_dir):\n",
    "    arrows = [os.path.join(arrow_dir, f) for f in os.listdir(arrow_dir) if f.endswith(\".arrow\")]\n",
    "    processed_dataset = hf_datasets.concatenate_datasets([hf_datasets.Dataset.from_file(arrow) for arrow in arrows]).to_dict()\n",
    "\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b55fa0-aabe-4327-ac1f-b59a26e9d07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aug_med_notes', 'mimic4'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets = {key: pd.DataFrame(load_labeled(processed_dir)) for key, processed_dir in processed_dirs.items()}\n",
    "processed_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03decef-49ea-4bfe-98be-b5765f7876e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:20:23,829 || INFO || MedRL-CoT Preprocess - Found 29654 rows\n",
      "2025-06-04 07:20:23,832 || INFO || MedRL-CoT Preprocess - Fixed class naming for 282 rows (0.9509678289606799 %)\n",
      "2025-06-04 07:20:23,854 || INFO || MedRL-CoT Preprocess - Re-classified 18 classes as 'other', or 1073 rows (3.618398866931948 %)\n",
      "2025-06-04 07:20:23,855 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 571 rows (1.925541242328185 %)\n",
      "2025-06-04 07:20:23,856 || INFO || MedRL-CoT Preprocess - Dropped 169 invalid rows (0.5699062521076415 %)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "symptoms_labs      16680\n",
       "diagnosis           9112\n",
       "thought_process     2620\n",
       "other               1073\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mimic_preprocess(dataset):\n",
    "    cleaned_ds = dataset.copy()\n",
    "\n",
    "    # For loggin purposes\n",
    "    N = cleaned_ds.shape[0]\n",
    "    logger.info(f\"Found {N} rows\")\n",
    "    num_renames = cleaned_ds[cleaned_ds['class'].isin(['symptoms_lbs', 'symptoms_lads'])].shape[0]\n",
    "    logger.info(f\"Fixed class naming for {num_renames} rows ({(num_renames / N)*100} %)\")\n",
    "    \n",
    "    # Fix naming of some classes\n",
    "    cleaned_ds['class'] = cleaned_ds['class'].replace({'symptoms_lbs': 'symptoms_labs', 'symptoms_lads': 'symptoms_labs'})\n",
    "    \n",
    "    pos_invalids = cleaned_ds[~cleaned_ds['class'].isin(classes)]\n",
    "    swapped_values = pos_invalids[pos_invalids['sentence'].str.lower().isin(classes)]    # Rows with swapped values\n",
    "    \n",
    "    # Clearly invalids, temp drop to remove from our ceaned list\n",
    "    invalid_classes = pos_invalids[pos_invalids['class'].str.lower().isin(['', '0', '__', 'None'])]  # collect empty sentence and classes (Note that doing it here will catch the invalid swapped sentences as well)\n",
    "    invalid_sentences = pos_invalids[pos_invalids['sentence'].str.lower().isin(['', '__', 'None'])]\n",
    "    invalids = pos_invalids.loc[invalid_classes.index.union(invalid_sentences.index)]\n",
    "    nonstd_classes = pos_invalids.drop(index=swapped_values.index.union(invalids.index))   # Get list of non-standard classes\n",
    "\n",
    "    # Get list of classes that can be classified as \"other\" with enough occurence (non-outliery)\n",
    "    # value_cnts = nonstd_classes['class'].value_counts()\n",
    "    # other_classes = value_cnts[value_cnts >= 5].index.tolist()\n",
    "    value_cnts = nonstd_classes['class'].value_counts()\n",
    "    other_classes = value_cnts[value_cnts >= 5].index.tolist()\n",
    "    other_class_indices = nonstd_classes[nonstd_classes['class'].isin(other_classes)].index\n",
    "    # print(value_cnts[value_cnts >= 5])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'past_surgical_history'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'followup_instructions'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'demographic_data'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'past_surgical_history'])\n",
    "    \n",
    "    # Clean the dataset\n",
    "    swapped_indices = swapped_values.index\n",
    "    cleaned_ds.loc[swapped_indices, ['sentence', 'class']] = cleaned_ds.loc[swapped_indices, ['class', 'sentence']].values # swap the values in indices where it's swapped\n",
    "    cleaned_ds.loc[other_class_indices, 'class'] = 'other'\n",
    "    # cleaned_ds['class'] = cleaned_ds['class'].apply(lambda x: 'other' if x in other_classes else x)  # relabel non-standards to 'other'\n",
    "    drop_indices = cleaned_ds[~cleaned_ds['class'].isin(np.append(classes, 'other'))].index\n",
    "    cleaned_ds = cleaned_ds.drop(index=drop_indices) # drop all others that aren't in our list of classes + 'other'  (basically all invalids)\n",
    "\n",
    "    # Summary\n",
    "    num_reclass = cleaned_ds[cleaned_ds['class'] == 'other'].shape[0]\n",
    "    logger.info(f\"Re-classified {len(other_classes)} classes as 'other', or {num_reclass} rows ({(num_reclass / N)*100} %)\")\n",
    "    logger.info(f'Swapped class and sentence values of {swapped_indices.shape[0]} rows ({(swapped_indices.shape[0] / N)*100} %)')\n",
    "    logger.info(f'Dropped {drop_indices.shape[0]} invalid rows ({(drop_indices.shape[0] / N)*100} %)')\n",
    "\n",
    "    # change case_id into int\n",
    "    cleaned_ds['case_id'] = cleaned_ds['case_id'].astype(int)\n",
    "\n",
    "    return cleaned_ds\n",
    "\n",
    "mimic_preprocess(processed_datasets['mimic4'])['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58e2d2b-b3ac-4d4a-86c2-4cecc94f1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:20:23,898 || INFO || MedRL-CoT Preprocess - Found 19968 rows\n",
      "2025-06-04 07:20:23,901 || INFO || MedRL-CoT Preprocess - Fixed class naming for 21 rows (0.10516826923076923 %)\n",
      "2025-06-04 07:20:23,917 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 131 rows (0.6560496794871795 %)\n",
      "2025-06-04 07:20:23,917 || INFO || MedRL-CoT Preprocess - Dropped 240 invalid rows (1.201923076923077 %)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "      <th>case_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A sixteen year-old girl, presented to our Outp...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She was not able to maintain an erect posture ...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She would keep her head turned to the right an...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was a sideways bending of the back in th...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To counter the abnormal positioning of the bac...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19963</th>\n",
       "      <td>No significant muscle atrophy was present, and...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19964</th>\n",
       "      <td>The physical examination showed that both of h...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19965</th>\n",
       "      <td>Both knees were very soft and could touch the ...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19966</th>\n",
       "      <td>Upon palpation, the continuity of the quadrice...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19967</th>\n",
       "      <td>In addition, the patient could not complete kn...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19728 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence          class  \\\n",
       "0      A sixteen year-old girl, presented to our Outp...  symptoms_labs   \n",
       "1      She was not able to maintain an erect posture ...  symptoms_labs   \n",
       "2      She would keep her head turned to the right an...  symptoms_labs   \n",
       "3      There was a sideways bending of the back in th...  symptoms_labs   \n",
       "4      To counter the abnormal positioning of the bac...  symptoms_labs   \n",
       "...                                                  ...            ...   \n",
       "19963  No significant muscle atrophy was present, and...  symptoms_labs   \n",
       "19964  The physical examination showed that both of h...  symptoms_labs   \n",
       "19965  Both knees were very soft and could touch the ...  symptoms_labs   \n",
       "19966  Upon palpation, the continuity of the quadrice...  symptoms_labs   \n",
       "19967  In addition, the patient could not complete kn...  symptoms_labs   \n",
       "\n",
       "       case_id  \n",
       "0           -1  \n",
       "1           -1  \n",
       "2           -1  \n",
       "3           -1  \n",
       "4           -1  \n",
       "...        ...  \n",
       "19963      912  \n",
       "19964      915  \n",
       "19965      915  \n",
       "19966      915  \n",
       "19967      915  \n",
       "\n",
       "[19728 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aug_preprocess(dataset):\n",
    "    cleaned_ds = dataset.copy()\n",
    "\n",
    "    # For loggin purposes\n",
    "    N = cleaned_ds.shape[0]\n",
    "    logger.info(f\"Found {N} rows\")\n",
    "    num_renames = cleaned_ds[cleaned_ds['class'].isin(['symptoms_lbs', 'symptoms_lads'])].shape[0]\n",
    "    logger.info(f\"Fixed class naming for {num_renames} rows ({(num_renames / N)*100} %)\")\n",
    "    \n",
    "    # Fix naming of some classes\n",
    "    cleaned_ds['class'] = cleaned_ds['class'].replace({'symptoms_lbs': 'symptoms_labs', 'symptoms_lads': 'symptoms_labs'})\n",
    "    \n",
    "    pos_invalids = cleaned_ds[~cleaned_ds['class'].isin(classes)]\n",
    "    swapped_values = pos_invalids[pos_invalids['sentence'].isin(classes)]    # Rows with swapped values\n",
    "    invalid_classes = pos_invalids[pos_invalids['class'].str.lower().isin(['', '0', '__', 'None', '[]', 'False', 'The', 'No'])]  # collect empty sentence and classes (Note that doing it here will catch the invalid swapped sentences as well)\n",
    "    ignore_classes = pos_invalids[pos_invalids['sentence'].str.contains('not a sentence')]\n",
    "    invalid_sentences = pos_invalids[pos_invalids['sentence'].str.lower().isin(['', '__', 'None', '[]', 'False', '()'])]\n",
    "\n",
    "    invalids = pos_invalids.loc[invalid_classes.index.union(invalid_sentences.index.union(ignore_classes.index))]\n",
    "    # invalids = pos_invalids.loc[invalid_classes.index.union(invalid_sentences.index)]\n",
    "    # invalids = pos_invalids[pos_invalids ['class'].isin(['', '0', '[]', 'False'])]   # Clearly invalids, temp drop to remove from our ceaned list\n",
    "    nonstd_classes = pos_invalids.drop(index=swapped_values.index.union(invalids.index))   # Get list of non-standard class rows\n",
    "    # print(ignore_classes.index[0] in list(nonstd_classes.index))\n",
    "    # print(ignore_classes.index[0] in list(invalids.index))\n",
    "    # print(nonstd_classes[nonstd_classes['sentence'].str.contains('not a sentence')])\n",
    "\n",
    "    # Get list of classes that can be classified as \"other\" with enough occurence (non-outliery)\n",
    "    # value_cnts = nonstd_classes['class'].value_counts()\n",
    "    # other_classes = value_cnts[value_cnts >= 5].index.tolist()\n",
    "    # other_class_rows = nonstd_classes[nonstd_classes['class'].isin(other_classes)]\n",
    "    # other_class_rows = other_class_rows[~other_class_rows['sentence'].str.contains('thought_process')]\n",
    "    # other_class_rows = other_class_rows[~other_class_rows['sentence'].str.contains('symptoms_labs')]\n",
    "    # other_class_indices = other_class_rows[~other_class_rows['sentence'].str.contains('diagnosis')].index  # Note we get rid of this because bad classification when it should've been \"diagnosis\", also removes \"diagnosis: \" sentences\n",
    "    # print(other_class_indices)\n",
    "    # print(value_cnts[value_cnts >= 5])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'symptoms_lbs'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'A'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'classification'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'No'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'The'])\n",
    "\n",
    "    swapped_indices = swapped_values.index\n",
    "    cleaned_ds.loc[swapped_indices, ['sentence', 'class']] = cleaned_ds.loc[swapped_indices, ['class', 'sentence']].values # swap the values in indices where it's swapped\n",
    "    # cleaned_ds.loc[other_class_indices, 'class'] = 'other'\n",
    "    # cleaned_ds['class'] = cleaned_ds['class'].apply(lambda x: 'other' if x in other_classes else x)  # relabel non-standards to 'other'\n",
    "    drop_indices = cleaned_ds[~cleaned_ds['class'].isin(classes)].index\n",
    "    # print(ignore_classes.index in list(drop_indices))\n",
    "    # print(ignore_classes.index)\n",
    "    # print(list(drop_indices))\n",
    "    \n",
    "    cleaned_ds = cleaned_ds.drop(index=drop_indices) # drop all others that aren't in our list of classes + 'other' (basically all invalids)\n",
    "\n",
    "    # Summary\n",
    "    num_reclass = cleaned_ds[cleaned_ds['class'] == 'other'].shape[0]\n",
    "    # logger.info(f\"Re-classified {len(other_classes)} classes as 'other', or {num_reclass} rows ({(num_reclass / N)*100} %)\")\n",
    "    logger.info(f'Swapped class and sentence values of {swapped_indices.shape[0]} rows ({(swapped_indices.shape[0] / N)*100} %)')\n",
    "    logger.info(f'Dropped {drop_indices.shape[0]} invalid rows ({(drop_indices.shape[0] / N)*100} %)')\n",
    "\n",
    "    # change case_id into int\n",
    "    cleaned_ds['case_id'] = cleaned_ds['case_id'].astype(int)\n",
    "    # print(cleaned_ds[cleaned_ds['sentence'].str.contains('not a sentence')])\n",
    "    \n",
    "    return cleaned_ds\n",
    "\n",
    "aug_preprocess(processed_datasets['aug_med_notes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5643b621-e1f5-48c4-b7b6-69a0c4569c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "proc_funcs = {'mimic4': mimic_preprocess, 'aug_med_notes': aug_preprocess}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc24bb2-7506-4e5a-9115-a2951271fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:20:23,982 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-04 07:20:23,983 || INFO || MedRL-CoT Preprocess - Cleaning up aug_med_notes dataset\n",
      "2025-06-04 07:20:23,985 || INFO || MedRL-CoT Preprocess - Found 19968 rows\n",
      "2025-06-04 07:20:23,986 || INFO || MedRL-CoT Preprocess - Fixed class naming for 21 rows (0.10516826923076923 %)\n",
      "2025-06-04 07:20:24,002 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 131 rows (0.6560496794871795 %)\n",
      "2025-06-04 07:20:24,002 || INFO || MedRL-CoT Preprocess - Dropped 240 invalid rows (1.201923076923077 %)\n",
      "2025-06-04 07:20:24,005 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-04 07:20:24,005 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-04 07:20:24,006 || INFO || MedRL-CoT Preprocess - Cleaning up mimic4 dataset\n",
      "2025-06-04 07:20:24,008 || INFO || MedRL-CoT Preprocess - Found 29654 rows\n",
      "2025-06-04 07:20:24,010 || INFO || MedRL-CoT Preprocess - Fixed class naming for 282 rows (0.9509678289606799 %)\n",
      "2025-06-04 07:20:24,031 || INFO || MedRL-CoT Preprocess - Re-classified 18 classes as 'other', or 1073 rows (3.618398866931948 %)\n",
      "2025-06-04 07:20:24,031 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 571 rows (1.925541242328185 %)\n",
      "2025-06-04 07:20:24,032 || INFO || MedRL-CoT Preprocess - Dropped 169 invalid rows (0.5699062521076415 %)\n",
      "2025-06-04 07:20:24,036 || INFO || MedRL-CoT Preprocess - ==================================================\n"
     ]
    }
   ],
   "source": [
    "preprocessed_datasets = dict()\n",
    "for key, item in processed_datasets.items():\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"Cleaning up {key} dataset\")\n",
    "    # processed_datasets[key] = mimic_preprocess(processed_datasets[key])\n",
    "    # mimic_preprocess(processed_datasets[key])\n",
    "    preprocessed_datasets[key] = proc_funcs[key](processed_datasets[key])\n",
    "    logger.info(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970f8205-b8fb-4fd7-8c7a-1bb540cd4085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aug_med_notes', 'mimic4'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4410b801-5e22-47e8-8a7b-89c99c484140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "        15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "        41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
       "        54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,\n",
       "        67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
       "        80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
       "        93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "       106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "       119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
       "       132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144,\n",
       "       145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
       "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
       "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
       "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "       197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
       "       223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
       "       236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
       "       249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261,\n",
       "       262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
       "       275, 276, 277, 278, 279, 280, 281, 282, 283])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_datasets['mimic4']['case_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073ba94c-9033-41e0-977e-3663c6e95118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def join_sentence_class(group):\n",
    "#     return ' '.join(f\"{row['sentence']} <{row['class']}>\" for _, row in group.iterrows())\n",
    "    \n",
    "# # preprocessed_datasets['mimic4'].groupby('case_id').apply(join_sentence_class).reset_index(name='full_class_case').sort_values('case_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81b6f92f-91dc-48cd-bbdc-03b29b06270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_datasets = dict()\n",
    "# for key, dataset in preprocessed_datasets.items():\n",
    "#     cases_datasets[key] = dataset.groupby('case_id').apply(join_sentence_class).reset_index(name='full_class_case').sort_values('case_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "091b4a97-8f38-4cdc-855f-74fa96912fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_datasets['aug_med_notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af634f5-3278-41c4-b4f2-ad4689448f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_datasets['aug_med_notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2daf529-4d0a-49f9-87e9-4baa1d6e738a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessed_datasets['aug_med_notes'][preprocessed_datasets['aug_med_notes']['class'] == 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a78ec6d9-3ab1-4522-9810-d475db8e3f17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:20:24,262 || INFO || Logger - Setup for MedRL-CoT's log done. This is the beginning of the log.\n",
      "2025-06-04 07:20:24,435 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-04 07:20:24,436 || INFO || MedRL-CoT Preprocess - Cleaning up aug_med_notes dataset\n",
      "2025-06-04 07:20:24,437 || INFO || MedRL-CoT Preprocess - Found 19968 rows\n",
      "2025-06-04 07:20:24,440 || INFO || MedRL-CoT Preprocess - Fixed class naming for 21 rows (0.10516826923076923 %)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new log file logs/medrlcot040.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:20:24,472 || INFO || MedRL-CoT Preprocess - Re-classified 5 classes as 'other', or 76 rows (0.38060897435897434 %)\n",
      "2025-06-04 07:20:24,473 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 131 rows (0.6560496794871795 %)\n",
      "2025-06-04 07:20:24,473 || INFO || MedRL-CoT Preprocess - Dropped 224 invalid rows (1.1217948717948718 %)\n",
      "2025-06-04 07:20:24,474 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-04 07:20:24,475 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-04 07:20:24,476 || INFO || MedRL-CoT Preprocess - Cleaning up mimic4 dataset\n",
      "2025-06-04 07:20:24,478 || INFO || MedRL-CoT Preprocess - Found 29654 rows\n",
      "2025-06-04 07:20:24,481 || INFO || MedRL-CoT Preprocess - Fixed class naming for 282 rows (0.9509678289606799 %)\n",
      "2025-06-04 07:20:24,512 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 571 rows (1.925541242328185 %)\n",
      "2025-06-04 07:20:24,513 || INFO || MedRL-CoT Preprocess - Dropped 1461 invalid rows (4.9268226883388415 %)\n",
      "2025-06-04 07:20:24,514 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "/tmp/ipykernel_26821/546570551.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cases_datasets[key] = dataset.groupby('case_id').apply(mp.xy_split_processing_sft).reset_index().sort_values('case_id')\n",
      "/tmp/ipykernel_26821/546570551.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cases_datasets[key] = dataset.groupby('case_id').apply(mp.xy_split_processing_sft).reset_index().sort_values('case_id')\n",
      "2025-06-04 07:20:27,049 || INFO || DataManager - Loading datasets: ['aug_med_notes', 'mimic4']\n",
      "2025-06-04 07:20:27,050 || INFO || DataManager - AGBonnet/augmented-clinical-notes dataset already exists in disk. If the dataset is giving errors or you'd like a fresh install, delete the /home/shared/medrlcot/data/aug_med_notes/train directory.\n",
      "2025-06-04 07:20:27,051 || INFO || DataManager - Loading saved hugginface AGBonnet/augmented-clinical-notes dataset.\n",
      "2025-06-04 07:20:27,052 || ERROR || DataManager - Error loading AGBonnet/augmented-clinical-notes dataset from local!\n",
      "2025-06-04 07:20:27,053 || INFO || DataManager - discharge.csv.gz dataset already exists in disk as huggin_face dataset. If the dataset is giving errors or you'd like a fresh install, delete the /home/shared/medrlcot/data/mimic4/hf directory.\n",
      "2025-06-04 07:20:27,053 || INFO || DataManager - Loading saved hugginface discharge.csv.gz dataset.\n",
      "2025-06-04 07:20:27,116 || INFO || DataManager - Successfully loaded hugging_face of discharge.csv.gz as key mimic4\n",
      "2025-06-04 07:20:27,117 || INFO || DataManager - Successfully loaded 2 datasets: ['aug_med_notes', 'mimic4']\n"
     ]
    }
   ],
   "source": [
    "def xy_split_processing_sft(group, x_func=None, y_func=None):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        if row['class'] == 'symptoms_labs' or row['class'] == 'other':\n",
    "            X.append(row)\n",
    "        else:\n",
    "            Y.append(row)\n",
    "\n",
    "    # X_case = ' '.join(f\"{row['sentence']} <{row['class']}>\" for _, row in group.iterrows()) # Input with all\n",
    "    X_case = x_func(X) if x_func else ' '.join([str(row['sentence']) for row in X])\n",
    "    Y_case = ' '.join([f\"{row['sentence']} <{row['class']}> \" for row in Y])    # Output with only thought_process and diagnosis\n",
    "    \n",
    "    X_prompt = f\"\"\"Below is a clinical case. Your task is to provide a step-by-step clinical reasoning followed by the diagnosis.\n",
    "\n",
    "    {X_case}\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.Series({'X': X_prompt, 'Y': Y_case})\n",
    "\n",
    "import medrlcot.preprocessing as mp\n",
    "preprocessed_datasets = mp.preprocess_datasets()\n",
    "\n",
    "# Combine cases into one for cases as example for SFT\n",
    "cases_datasets = dict()\n",
    "for key, dataset in preprocessed_datasets.items():\n",
    "    cases_datasets[key] = dataset.groupby('case_id').apply(mp.xy_split_processing_sft).reset_index().sort_values('case_id')\n",
    "    cases_datasets[key]['wc_x'] = cases_datasets[key]['X'].apply(lambda x: len(x.split())) \n",
    "    cases_datasets[key]['wc_y'] = cases_datasets[key]['Y'].apply(lambda x: len(x.split())) \n",
    "    cases_datasets[key] = cases_datasets[key][(cases_datasets[key]['wc_x'] <= 800) & (cases_datasets[key]['wc_y'] <= 900)].reset_index(drop=True)\n",
    "\n",
    "import os\n",
    "from medrlcot import data_manager\n",
    "import medrlcot.config.env as mce\n",
    "model_cfg_path = os.path.join(os.getcwd(), \"medrlcot/config/.env\")\n",
    "medrlcot_config = mce.MedRL_CoT(model_cfg_path)\n",
    "raw_datasets = data_manager.load_datasets(medrlcot_config.datasets, data_dir=medrlcot_config.data_dir)  # Load raw dataset (original cases) for RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d02ed4d7-95df-49bd-9597-b9e44a139564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>wc_x</th>\n",
       "      <th>wc_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>A 34 year old Persian woman, gravida 1, para 0...</td>\n",
       "      <td>159</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>She was treated with thrombolysis and endovasc...</td>\n",
       "      <td>180</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>classification [diagnosis]</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>Staged bilateral total hip arthroplasties were...</td>\n",
       "      <td>374</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The initial workup was unrevealing for an etio...</td>\n",
       "      <td>248</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>93</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>A 45 mm × 37 mm pseudoaneurysm in lateral side...</td>\n",
       "      <td>329</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>94</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The working diagnosis was a collection seconda...</td>\n",
       "      <td>147</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>96</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>She had undergone endovascular trapping of the...</td>\n",
       "      <td>160</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>97</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The patient underwent general anesthesia for t...</td>\n",
       "      <td>287</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>98</td>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The patient was diagnosed with spinal muscular...</td>\n",
       "      <td>151</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>737 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    case_id                                                  X  \\\n",
       "0       100  Provide a step-by-step clinical reasoning foll...   \n",
       "1       101  Provide a step-by-step clinical reasoning foll...   \n",
       "2       102  Provide a step-by-step clinical reasoning foll...   \n",
       "3       103  Provide a step-by-step clinical reasoning foll...   \n",
       "4       104  Provide a step-by-step clinical reasoning foll...   \n",
       "..      ...                                                ...   \n",
       "732      93  Provide a step-by-step clinical reasoning foll...   \n",
       "733      94  Provide a step-by-step clinical reasoning foll...   \n",
       "734      96  Provide a step-by-step clinical reasoning foll...   \n",
       "735      97  Provide a step-by-step clinical reasoning foll...   \n",
       "736      98  Provide a step-by-step clinical reasoning foll...   \n",
       "\n",
       "                                                     Y  wc_x  wc_y  \n",
       "0    A 34 year old Persian woman, gravida 1, para 0...   159   304  \n",
       "1    She was treated with thrombolysis and endovasc...   180   255  \n",
       "2                          classification [diagnosis]     12     2  \n",
       "3    Staged bilateral total hip arthroplasties were...   374   314  \n",
       "4    The initial workup was unrevealing for an etio...   248   149  \n",
       "..                                                 ...   ...   ...  \n",
       "732  A 45 mm × 37 mm pseudoaneurysm in lateral side...   329   185  \n",
       "733  The working diagnosis was a collection seconda...   147   415  \n",
       "734  She had undergone endovascular trapping of the...   160   587  \n",
       "735  The patient underwent general anesthesia for t...   287   183  \n",
       "736  The patient was diagnosed with spinal muscular...   151    77  \n",
       "\n",
       "[737 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_datasets['aug_med_notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bce3bcdf-2c5e-4959-b97b-9c617082eeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "symptoms_labs      9977\n",
       "thought_process    5318\n",
       "diagnosis          4373\n",
       "other                76\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_datasets['aug_med_notes']['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d265d0e4-663a-49c2-b5ca-050e0567eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_datasets = dict()\n",
    "# for key, dataset in cases_datasets.items():\n",
    "#     dataset['wc_x'] = dataset['X'].apply(lambda x: len(x.split())) \n",
    "#     dataset['wc_y'] = dataset['Y'].apply(lambda x: len(x.split())) \n",
    "#     filtered_datasets[key] = dataset[(dataset['wc_x'] <= 800) & (dataset['wc_y'] <= 900)].reset_index(drop=True)\n",
    "\n",
    "# print(cases_datasets['aug_med_notes'].shape, filtered_datasets['aug_med_notes'].shape)\n",
    "# print(cases_datasets['mimic4'].shape, filtered_datasets['mimic4'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d61fa9a-274c-4cd9-b23b-975b641570ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('mimic4', (47, case_id                                                  152\n",
      "X          Provide a step-by-step clinical reasoning foll...\n",
      "Y          She received 1L NS, 5 mg of morphine, 25 mg of...\n",
      "wc_x                                                     795\n",
      "wc_y                                                     362\n",
      "Name: 47, dtype: object)), 795) (('mimic4', (191, case_id                                                   56\n",
      "X          Provide a step-by-step clinical reasoning foll...\n",
      "Y          Thought process: logical, linear, perservated ...\n",
      "wc_x                                                     793\n",
      "wc_y                                                     527\n",
      "Name: 191, dtype: object)), 793)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = (None, 0)\n",
    "next_max = (None, -1)\n",
    "num_exceed = 0\n",
    "for key, dataset in cases_datasets.items():\n",
    "    for row in dataset.iterrows():\n",
    "        cnt = ((key, row), len(row[1]['X'].split()))\n",
    "        if cnt[1] > 750:\n",
    "            num_exceed += 1\n",
    "        if cnt[1] > max_words[1]:\n",
    "            next_max = max_words\n",
    "            max_words = cnt\n",
    "        else:\n",
    "            next_max = max(cnt, next_max, key=lambda x: x[1])\n",
    "print(max_words, next_max)\n",
    "num_exceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ff63a06-c251-403a-b97f-3eec8fd85fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:20:27,711 || INFO || DataManager - Using train-val split: [0.75, 0.25]\n",
      "2025-06-04 07:20:27,712 || INFO || DataManager - Split-Shuffle with seed 0\n",
      "2025-06-04 07:20:27,713 || INFO || DataManager - Splitting aug_med_notes dataset.\n",
      "2025-06-04 07:20:27,715 || INFO || DataManager - Split into 552 train rows and 185 val rows\n",
      "2025-06-04 07:20:27,716 || INFO || DataManager - Splitting mimic4 dataset.\n",
      "2025-06-04 07:20:27,718 || INFO || DataManager - Split into 174 train rows and 59 val rows\n",
      "2025-06-04 07:20:27,719 || INFO || DataManager - Creating a single joint dataset of dict_keys(['aug_med_notes', 'mimic4']) dataset splits\n",
      "2025-06-04 07:20:27,721 || INFO || DataManager - Joined 726 train rows\n",
      "2025-06-04 07:20:27,722 || INFO || DataManager - Shuffling train's rows\n",
      "2025-06-04 07:20:27,723 || INFO || DataManager - Joined 244 val rows\n",
      "2025-06-04 07:20:27,724 || INFO || DataManager - Shuffling val's rows\n",
      "2025-06-04 07:20:27,726 || INFO || DataManager - Returning shuffled train-val splits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='int64')\n",
      "Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Create customd dataset obj\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n",
    "cases_data = data_manager.MedRL_CoT_Dataset(cases_datasets, seed=0, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c93759c-e3bf-477f-9a9a-57e8cde169f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_data['train'] = cases_data['train'].drop(cases_data['train'][cases_data['train']['Y'].str.strip() == ''].index)\n",
    "# print(cases_data['train'][cases_data['train']['X'].str.strip() == ''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "091bc68c-8049-4a1c-bd20-71d1b4962f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>Pain of the first mentioned episode was subsid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The swelling was seen extending from mesial as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The patient was diagnosed as TN at another hos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>After being referred to several gastroenterolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>While mast cells represent the effector cell i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>Discharge Disposition: [diagnosis]  Discharge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>Sinus plain radiographs demonstrated ‘sinusiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>After admission, the patient maintained with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>An 80-year-old woman with a history of collaps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The patient declined the option of removing th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     X  \\\n",
       "0    Provide a step-by-step clinical reasoning foll...   \n",
       "1    Provide a step-by-step clinical reasoning foll...   \n",
       "2    Provide a step-by-step clinical reasoning foll...   \n",
       "3    Provide a step-by-step clinical reasoning foll...   \n",
       "4    Provide a step-by-step clinical reasoning foll...   \n",
       "..                                                 ...   \n",
       "721  Provide a step-by-step clinical reasoning foll...   \n",
       "722  Provide a step-by-step clinical reasoning foll...   \n",
       "723  Provide a step-by-step clinical reasoning foll...   \n",
       "724  Provide a step-by-step clinical reasoning foll...   \n",
       "725  Provide a step-by-step clinical reasoning foll...   \n",
       "\n",
       "                                                     Y  \n",
       "0    Pain of the first mentioned episode was subsid...  \n",
       "1    The swelling was seen extending from mesial as...  \n",
       "2    The patient was diagnosed as TN at another hos...  \n",
       "3    After being referred to several gastroenterolo...  \n",
       "4    While mast cells represent the effector cell i...  \n",
       "..                                                 ...  \n",
       "721  Discharge Disposition: [diagnosis]  Discharge ...  \n",
       "722  Sinus plain radiographs demonstrated ‘sinusiti...  \n",
       "723  After admission, the patient maintained with a...  \n",
       "724  An 80-year-old woman with a history of collaps...  \n",
       "725  The patient declined the option of removing th...  \n",
       "\n",
       "[716 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57ba1d0d-f808-4f19-8e55-3a367692ead4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [X, Y]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data['train'][cases_data['train']['Y'].str.strip() == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6577f860-e2bb-45e2-b632-1226e0a05543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>Pain of the first mentioned episode was subsid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The swelling was seen extending from mesial as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The patient was diagnosed as TN at another hos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>After being referred to several gastroenterolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>While mast cells represent the effector cell i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>Discharge Disposition: [diagnosis]  Discharge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>Sinus plain radiographs demonstrated ‘sinusiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>After admission, the patient maintained with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>An 80-year-old woman with a history of collaps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>Provide a step-by-step clinical reasoning foll...</td>\n",
       "      <td>The patient declined the option of removing th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     X  \\\n",
       "0    Provide a step-by-step clinical reasoning foll...   \n",
       "1    Provide a step-by-step clinical reasoning foll...   \n",
       "2    Provide a step-by-step clinical reasoning foll...   \n",
       "3    Provide a step-by-step clinical reasoning foll...   \n",
       "4    Provide a step-by-step clinical reasoning foll...   \n",
       "..                                                 ...   \n",
       "721  Provide a step-by-step clinical reasoning foll...   \n",
       "722  Provide a step-by-step clinical reasoning foll...   \n",
       "723  Provide a step-by-step clinical reasoning foll...   \n",
       "724  Provide a step-by-step clinical reasoning foll...   \n",
       "725  Provide a step-by-step clinical reasoning foll...   \n",
       "\n",
       "                                                     Y  \n",
       "0    Pain of the first mentioned episode was subsid...  \n",
       "1    The swelling was seen extending from mesial as...  \n",
       "2    The patient was diagnosed as TN at another hos...  \n",
       "3    After being referred to several gastroenterolo...  \n",
       "4    While mast cells represent the effector cell i...  \n",
       "..                                                 ...  \n",
       "721  Discharge Disposition: [diagnosis]  Discharge ...  \n",
       "722  Sinus plain radiographs demonstrated ‘sinusiti...  \n",
       "723  After admission, the patient maintained with a...  \n",
       "724  An 80-year-old woman with a history of collaps...  \n",
       "725  The patient declined the option of removing th...  \n",
       "\n",
       "[716 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "964153c6-e86d-4aa6-960e-7eed5a27ba4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pain of the first mentioned episode was subsided spontaneously and the patient underwent endoscopic retrograde cholangiopancreatography (ERCP) 1 month later [diagnosis]  but there was no visible CBD stone on ERCP at that time. [diagnosis]  Differential diagnosis made by CT scan was a polypoid mass arising from biliary ducts or thick sludge fulfilling biliary ducts. [diagnosis]  classification [diagnosis]  Hence, tissue sampling of mentioned infiltrative mass was performed in May 2019 revealing cholangiocarcinoma developing in intraductal papillary neoplasm of bile duct in microscopic pathological study and mucin secreting neoplasm (adenocarcinoma) with GI origin on immunohistochemistry study. [diagnosis]  Another contrast enhanced abdominal CT scan was ordered in May, 2019 before surgery [] which demonstrates heterogeneous mass in size of 70 mm × 42 mm at left liver lobe accompanied with perilesional staining and focal peripheral biliary duct ectasia. [thought_process]  Branching and extension of mentioned mass were seen through left hepatic duct without involvement of portal or hepatic veins, with mild enlargement in size in comparison to previous study. [thought_process] '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data['train']['Y'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ac83011-3165-4c21-81f8-3ae069bd813a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7697466c1090>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data.get_dataloader('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2524fc2-3421-43ef-a89a-4c2e3167d7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7697466c2620>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data.get_dataloader('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44c1b78f-0f78-4011-9270-b3e3ecfe14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model  = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f88ea78e-f58c-4e93-a078-dd644f267828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b18df7ed-69a4-4be2-95ea-3c03fa9ff047",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0d9e217-fc17-4cdd-a578-070737acec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds): # https://www.datacamp.com/tutorial/flan-t5-tutorial\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "420d2d61-1b6b-4328-9a70-9faa9544a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok_dataset = cases_data.get_torch_dataset('train')\n",
    "val_tok_dataset = cases_data.get_torch_dataset('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98f90dff-e9e7-4dd2-b34a-88c74f9295f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14298,   769, 18780,    23,   152,   592,    47, 13090,   227,     3,\n",
       "            9,  6743,    47,   990,     5,   784, 11841,    17,   834, 15056,\n",
       "          908,    37,   255,     9,   189,    47,   646,    16,   286,    11,\n",
       "            8,  3979,    47,   703,   127,  1054,     5,   784, 11841,    17,\n",
       "          834, 15056,   908,    37,  1868,    47, 24731, 16928,  1427,  5711,\n",
       "           11,  1026,    12, 29216,    26,    12,    51,  5984,    41,  6227,\n",
       "           61, 19083,    21,  5002,    13, 27970,  2871,     5,   784, 11841,\n",
       "           17,   834, 15056,   908,    37,  8668,  5924,  5111,  1223,    54,\n",
       "           29,  7830,    13,     8,     3,     9,   127,  1225, 26806,    28,\n",
       "            8,   255,     9,   189,     5,   784, 25930,  4844,   159,   908,\n",
       "           37,  1868,    47,  1461, 10250,    57, 16352,    12,    69,  6568,\n",
       "           21,  1146,   593,    13,     3, 15100,  3730,   124,     5,   784,\n",
       "        11841,    17,   834, 15056,   908,    37,  1868,    47,  1026,    12,\n",
       "            8,  9279,  2699,   562,    16,    69,  3064,     5,   784, 11841,\n",
       "           17,   834, 15056,   908,  2106, 12088,   399, 31904,     3,    89,\n",
       "           15, 21511,   592,    47,  5105,    11,     3,     9,   874,  2379,\n",
       "          255,     9,   189,    47,  2681,    16,     8,   269,  1017,     3,\n",
       "           89,    15, 21511,     3, 27845,     6,    11,  7513,     6,     3,\n",
       "            9,  2391,  2379,   255,     9,   189,    47,  2681,    16,     8,\n",
       "          646,  1017,     3,    89,    15, 21511,     3, 27845,     5,   784,\n",
       "        11841,    17,   834, 15056,   908,   101,  2681,   192,   813,  4707,\n",
       "          221,  1904,    16,     8,   646,   592,    16,  1068,   554, 16221,\n",
       "         3317,    38,    48,    47,    69, 12723,   596,    13,  1407, 12001,\n",
       "            5,   784, 11841,    17,   834, 15056,   908,   486,    48,   500,\n",
       "            6,     3,     9,     3,  9905,  5756, 27594,    47,  2496,   139,\n",
       "            8, 25200,    53,     3,     9,   127,    17,     9,    11,    46,\n",
       "        11508,     3,     9,   127,    17, 16275,    47,  3032,    84,  9028,\n",
       "            3,     9,  1223,    54,    29,  7830,    13,     8,     3,     9,\n",
       "          127,  1225, 26806,    41,   137,   784, 11841,    17,   834, 15056,\n",
       "          908,   101,  8160,    12,  8669,    12,    28,     3,     7,  4669,\n",
       "            3,  8290,    17,  7907,    12,  1865,    48,  2871,    38,     8,\n",
       "         1868,    22,     7,     3,   287,   127,  9824,  2197,    11,  2496,\n",
       "         1246,   130, 19551,   757,  5217,    21,   539,  2096,     5,   784,\n",
       "        11841,    17,   834, 15056,   908,  6783,    12,     8,  7907,    13,\n",
       "           46,   414,    32,  1409,     7, 24874,     6,     3,     9, 24387,\n",
       "           46, 10253,  5096,    47,  3032,    12,  2862, 14486,  5702, 15100,\n",
       "        10138,   663,     5,   784, 11841,    17,   834, 15056,   908,   100,\n",
       "         9028,     3,     9, 12613,   269, 22423,  1939,    40,     3, 27845,\n",
       "           11,   646, 22423,  1939,    40,    44,    60,     7,    23,     9,\n",
       "            5,   784, 11841,    17,   834, 15056,   908,   290,    47,  9289,\n",
       "         2357,    45,     8,   353,    13,   255,     9,   189,     3, 23565,\n",
       "           12,     8,   646,   769, 18780,    23,   152,     3, 27845,     6,\n",
       "          224,    24,    62,  1800,    62,   228,  1984,     3,     9,  7042,\n",
       "          406,  6013,     8,   646,   769, 18780,    23,   152,     3, 27845,\n",
       "            5,   784, 11841,    17,   834, 15056,   908,  6719,    30,    69,\n",
       "          554, 11480,  9753,    13,    69, 12723,     3, 20042,    23,  1982,\n",
       "           11,  1028,  1947,  7042, 10029,     6,     8,     3,     9,   127,\n",
       "         1225,  9260,    47,  3538,     3,   635,     5,   784, 11841,    17,\n",
       "          834, 15056,   908,  4063,     6,    62,  2639,     3,     9,  4475,\n",
       "            3,   635,     3,     2, 11321,     3,   635,  2964,  9333,     3,\n",
       "           17, 21783, 13241,     3,     9,   127,  1225,     3,     7,  4669,\n",
       "            3,     1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = val_tok_dataset[100]\n",
    "t['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01c5e930-b3bb-4530-8824-0b2b2df99d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provide a step-by-step clinical reasoning followed by the diagnosis: An 88-year-old man was transferred from a referring hospital for descending thoracic aortic injury after attempted pacemaker placement; 3 days prior he was admitted with a transient ischemic attack. He had new onset atrial fibrillation and sinus bradycardia that prompted pacemaker placement. After placement of a five French sheath, arterial blood return was noted. The patient arrived intubated and sedated with the sheath in place in the left chest covered with a dressing and a left chest tube in place with 100cc of sanguinous output. Thought Process: Diagnosis: </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(t['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83f755d0-2006-41a9-af94-b33df58d7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aortic aortic injury\n"
     ]
    }
   ],
   "source": [
    "inputs = val_tok_dataset[100]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0).to(model.device)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=200)\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70336c3a-23e2-4790-9b71-5ed21253f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a step-by-step clinical reasoning followed by the diagnosis: In December 2006, about 6 months earlier than the first episode, checkup ultrasound had been done for the patient that showed only mild focal fatty change in lateral segments of left liver lobe without similar findings of biliary ducts. An abdominal computed tomography (CT) scan was performed on June 2011, (images not provided) which demonstrates a hypodense mass-like lesion in the origin of the left hepatic duct measuring 38 mm <unk> 18 mm <unk> 34 mm with extension to peripheral branches causing dilation of left main hepatic duct and left intrahepatic biliary ducts. At this time (September 2018) MRCP was ordered for more evaluation. MRCP demonstrates a T2 high signal intensity lesion in the left liver lobe accompanied with intrahepatic biliary duct ectasia and parenchymal shrinkage []. EUS was done in March, 2019 which demonstrates a 50 mm <unk> 30 mm hypoechoic mass in the left liver lobe containing dilated intrahepatic ducts up to 6 mm. The CBD was unremarkable and measured 6 mm in the proximal part. The patient underwent surgery, which gross pathology findings consisted of an intraparenchymal mass in the left liver lobe with the largest gross specimen measuring 3 cm. ['Microscopic pathological findings of resected mass were consistent with cholangiocarcinoma with involvement of regional lymph nodes []'] Thought Process: Diagnosis: </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "tensor([19043,    13,     8,   166,  2799,  5640,    47,   769, 17389, 23496,\n",
      "          120,    11,     8,  1868,   365, 16103,   414, 23643,  9337,  6801,\n",
      "            3, 14297,   152, 10253,  2837,  5045,   144,  5984,    41,  3316,\n",
      "         4184,    61,   209,   847,   865,   784, 25930,  4844,   159,   908,\n",
      "           68,   132,    47,   150,  5183, 12080,  3372,    30,     3,  3316,\n",
      "         4184,    44,    24,    97,     5,   784, 25930,  4844,   159,   908,\n",
      "        13936,    23,   138,  8209,   263,    57,  8668,  5924,    47,     3,\n",
      "            9,  4251,   102,    32,    23,    26,  3294,     3, 14739,    45,\n",
      "            3,  3727,    23,  1208,     3,  7472,     7,    42,  4126,     3,\n",
      "            7,    40, 13164, 19697,     3,  3727,    23,  1208,     3,  7472,\n",
      "            7,     5,   784, 25930,  4844,   159,   908, 13774,   784, 25930,\n",
      "         4844,   159,   908,     3, 13151,     6,  6316, 17222,    13,  2799,\n",
      "           16,  8027,  1313,  3268,  3294,    47,  3032,    16,   932,  1360,\n",
      "            3, 20508,     3, 14297,   152, 10253,  1720, 21919,    51,     9,\n",
      "         2421,    16,  6344,  7472,   138,  2576, 12851,    63,     3,    29,\n",
      "           15,    32, 21178,    13,     3,  3727,    15,     3,  7472,    16,\n",
      "         6647,    52, 23643,  2071,  4478,   810,    11,  4035,    75,    77,\n",
      "         2829,    53,     3,    29,    15,    32, 21178,    41,     9,   537,\n",
      "           32,  1720, 21919,    51,     9,    61,    28,     3,  7214,  5233,\n",
      "           30, 17133, 10193,   235, 11366,   810,     5,   784, 25930,  4844,\n",
      "          159,   908,  2351,  4656,  8358, 22827,  8668,  5924,    47,  5563,\n",
      "           16,   932,     6,  1360,   274,  3730,   784,   908,    84,     3,\n",
      "        21275, 26481,   729,    15,  1162,  3294,    16,   812,    13,  2861,\n",
      "            3,   635,     3,     2,  6426,     3,   635,    44,   646, 11501,\n",
      "            3, 11846,    15,     3, 10102,    28,   399,   699,     7,  6318,\n",
      "         3342,    77,    53,    11, 15949, 22586,     3,  3727,    23,  1208,\n",
      "            3,  7472,     3,    15,    75,    17, 15974,     5,   784, 11841,\n",
      "           17,   834, 15056,   908, 14009,    53,    11,  4924,    13,  2799,\n",
      "         3294,   130,   894,   190,   646,     3,    88,  7768,    75,     3,\n",
      "         7472,   406,  9683,    13,  8948,    42,     3,    88,  7768,    75,\n",
      "        13290,     7,     6,    28,  8248,     3,    35, 21933,    16,   812,\n",
      "           16,  4993,    12,  1767,   810,     5,   784, 11841,    17,   834,\n",
      "        15056,   908,     3,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "sample = train_tok_dataset[0]\n",
    "print(tokenizer.decode(sample[\"input_ids\"]))\n",
    "print(sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39ca4695-8d06-4388-bfa2-6e673f19c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_tok_dataset,\n",
    "    eval_dataset=val_tok_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c91efb80-a238-4605-8d69-e00f2ee73a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.027536630630493\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sample = train_tok_dataset[0]\n",
    "batch = {k: v.unsqueeze(0).to(model.device) for k, v in sample.items()}\n",
    "with torch.no_grad():\n",
    "    output = model(**batch)\n",
    "print(\"Loss:\", output.loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c340c6d3-9b7b-442a-b23b-082ef8449433",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9653c-0e0c-4e1b-90f6-f7a1d078478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Pika-Sannnnn/.local/lib/python3.10/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='369' max='2148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 369/2148 04:49 < 23:23, 1.27 it/s, Epoch 0.51/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080a57a-bc1c-493d-a64e-3a6af5dc7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_case = cases_data['val'].iloc[-1]\n",
    "\n",
    "# val_input = example_case[\"X\"]\n",
    "# val_target = example_case[\"Y\"]\n",
    "# val_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cca948-a336-4a7e-9603-a02711df3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tok_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa52eb-3f42-4676-847d-601ce82ffdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69341638-c971-492b-920a-fe8ab793d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(val_tok_dataset[100]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0a0ce-3cf2-420f-b2f8-e6bf737d7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = val_tok_dataset[99]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0).to(model.device)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=200)\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1d4bb-c6d9-4a71-a35a-eaa11746b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_case = \"\"\"\n",
    "# John Doe is a 40-year-old male who was involved in a motor vehicle accident. \n",
    "# He was a restrained passenger who was rear-ended by another car going approximately 45 mph. \n",
    "# He has no medical history and takes no medications at home. \n",
    "# He has no allergies. His primary physician is Dr. Johnson. He has been awake, alert, and oriented with a Glasgow Coma Score of 15 since arrival. \n",
    "# He is moving all extremities and has good strength bilaterally. \n",
    "# His chief complaint is neck pain, rated a 5 out of 10, and he remains in cervical-spine precautions until the trauma team clears him. \n",
    "# He is to be kept NPO until cleared by the trauma team as well. CT scans have been completed of the head, cervical spine, chest, abdomen, and pelvis. \n",
    "# We are pending reports from radiology. Last vital signs, 15 minutes ago, were temperature 36.6, pulse 80, respiratory rate 14, and blood pressure 120/80. Lung sounds are clear.\n",
    "# The abdomen is soft and non-tender. The patient has two largebore IVs. The Right AC 18 gauge with 1 liter Lactated Ringers at TKO. \n",
    "# The left AC has been saline locked. The patient received fentanyl 50 mcg slow IVP, and stated relief, with pain now 1 out of 10. \n",
    "# The family is at the bedside, and the patient remains in good spirits.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10379d3d-3348-4258-aa46-4c7113516f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
