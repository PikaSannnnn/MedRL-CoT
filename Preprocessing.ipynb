{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89d5679-991e-4f17-bcdf-a39beef3ef0e",
   "metadata": {},
   "source": [
    "### Preprocessing for Mimic4 Dataset\n",
    "\n",
    "**Note that the AUG dataset is incomplete, so we skip it here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ad5779-a5ac-4867-9b7f-fc3023047b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e50ca3-9850-42d7-b71f-8390c9a90dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medrlcot.config.env import MedRL_CoT\n",
    "from medrlcot import data_manager\n",
    "from medrlcot.medrlcot_logger import setup_logger\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Features, Value\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets as hf_datasets\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)  \n",
    "# pd.set_option('display.width', 0)    \n",
    "# pd.set_option('display.max_rows', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a0e936-71fc-48ac-b099-6f3050346a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:08,528 || INFO || Logger - Setup for MedRL-CoT's log done. This is the beginning of the log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new log file logs/medrlcot121.log\n"
     ]
    }
   ],
   "source": [
    "model_cfg_path = os.path.join(os.getcwd(), \"medrlcot/config/.env\")\n",
    "medrlcot_config = MedRL_CoT(model_cfg_path)\n",
    "\n",
    "setup_logger()\n",
    "logger = logging.getLogger(\"MedRL-CoT Preprocess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c6e504-937a-4abe-9c61-aa5c59490b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aug_med_notes', 'mimic4'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed_dirs = [os.path.join(os.getcwd(), medrlcot_config.data_dir, ds, 'processed') for ds in medrlcot_config.datasets]\n",
    "processed_dirs = {ds: os.path.join(os.getcwd(), medrlcot_config.data_dir, ds, 'processed') for ds in medrlcot_config.datasets}\n",
    "processed_dirs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499d4653-c605-406c-ad24-a1580c7e1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(['symptoms_labs', 'thought_process', 'diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee7871d1-58d9-4dd2-86ed-e4b44df82554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labeled(arrow_dir):\n",
    "    arrows = [os.path.join(arrow_dir, f) for f in os.listdir(arrow_dir) if f.endswith(\".arrow\")]\n",
    "    processed_dataset = hf_datasets.concatenate_datasets([hf_datasets.Dataset.from_file(arrow) for arrow in arrows]).to_dict()\n",
    "\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b55fa0-aabe-4327-ac1f-b59a26e9d07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aug_med_notes', 'mimic4'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets = {key: pd.DataFrame(load_labeled(processed_dir)) for key, processed_dir in processed_dirs.items()}\n",
    "processed_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03decef-49ea-4bfe-98be-b5765f7876e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:08,943 || INFO || MedRL-CoT Preprocess - Found 29654 rows\n",
      "2025-06-03 18:54:08,947 || INFO || MedRL-CoT Preprocess - Fixed class naming for 282 rows (0.9509678289606799 %)\n",
      "2025-06-03 18:54:08,965 || INFO || MedRL-CoT Preprocess - Re-classified 18 classes as 'other', or 1073 rows (3.618398866931948 %)\n",
      "2025-06-03 18:54:08,966 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 571 rows (1.925541242328185 %)\n",
      "2025-06-03 18:54:08,967 || INFO || MedRL-CoT Preprocess - Dropped 169 invalid rows (0.5699062521076415 %)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "symptoms_labs      16680\n",
       "diagnosis           9112\n",
       "thought_process     2620\n",
       "other               1073\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mimic_preprocess(dataset):\n",
    "    cleaned_ds = dataset.copy()\n",
    "\n",
    "    # For loggin purposes\n",
    "    N = cleaned_ds.shape[0]\n",
    "    logger.info(f\"Found {N} rows\")\n",
    "    num_renames = cleaned_ds[cleaned_ds['class'].isin(['symptoms_lbs', 'symptoms_lads'])].shape[0]\n",
    "    logger.info(f\"Fixed class naming for {num_renames} rows ({(num_renames / N)*100} %)\")\n",
    "    \n",
    "    # Fix naming of some classes\n",
    "    cleaned_ds['class'] = cleaned_ds['class'].replace({'symptoms_lbs': 'symptoms_labs', 'symptoms_lads': 'symptoms_labs'})\n",
    "    \n",
    "    pos_invalids = cleaned_ds[~cleaned_ds['class'].isin(classes)]\n",
    "    swapped_values = pos_invalids[pos_invalids['sentence'].str.lower().isin(classes)]    # Rows with swapped values\n",
    "    \n",
    "    # Clearly invalids, temp drop to remove from our ceaned list\n",
    "    invalid_classes = pos_invalids[pos_invalids['class'].str.lower().isin(['', '0', '__', 'None'])]  # collect empty sentence and classes (Note that doing it here will catch the invalid swapped sentences as well)\n",
    "    invalid_sentences = pos_invalids[pos_invalids['sentence'].str.lower().isin(['', '__', 'None'])]\n",
    "    invalids = pos_invalids.loc[invalid_classes.index.union(invalid_sentences.index)]\n",
    "    nonstd_classes = pos_invalids.drop(index=swapped_values.index.union(invalids.index))   # Get list of non-standard classes\n",
    "\n",
    "    # Get list of classes that can be classified as \"other\" with enough occurence (non-outliery)\n",
    "    # value_cnts = nonstd_classes['class'].value_counts()\n",
    "    # other_classes = value_cnts[value_cnts >= 5].index.tolist()\n",
    "    value_cnts = nonstd_classes['class'].value_counts()\n",
    "    other_classes = value_cnts[value_cnts >= 5].index.tolist()\n",
    "    other_class_indices = nonstd_classes[nonstd_classes['class'].isin(other_classes)].index\n",
    "    # print(value_cnts[value_cnts >= 5])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'past_surgical_history'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'followup_instructions'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'demographic_data'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'past_surgical_history'])\n",
    "    \n",
    "    # Clean the dataset\n",
    "    swapped_indices = swapped_values.index\n",
    "    cleaned_ds.loc[swapped_indices, ['sentence', 'class']] = cleaned_ds.loc[swapped_indices, ['class', 'sentence']].values # swap the values in indices where it's swapped\n",
    "    cleaned_ds.loc[other_class_indices, 'class'] = 'other'\n",
    "    # cleaned_ds['class'] = cleaned_ds['class'].apply(lambda x: 'other' if x in other_classes else x)  # relabel non-standards to 'other'\n",
    "    drop_indices = cleaned_ds[~cleaned_ds['class'].isin(np.append(classes, 'other'))].index\n",
    "    cleaned_ds = cleaned_ds.drop(index=drop_indices) # drop all others that aren't in our list of classes + 'other'  (basically all invalids)\n",
    "\n",
    "    # Summary\n",
    "    num_reclass = cleaned_ds[cleaned_ds['class'] == 'other'].shape[0]\n",
    "    logger.info(f\"Re-classified {len(other_classes)} classes as 'other', or {num_reclass} rows ({(num_reclass / N)*100} %)\")\n",
    "    logger.info(f'Swapped class and sentence values of {swapped_indices.shape[0]} rows ({(swapped_indices.shape[0] / N)*100} %)')\n",
    "    logger.info(f'Dropped {drop_indices.shape[0]} invalid rows ({(drop_indices.shape[0] / N)*100} %)')\n",
    "\n",
    "    # change case_id into int\n",
    "    cleaned_ds['case_id'] = cleaned_ds['case_id'].astype(int)\n",
    "\n",
    "    return cleaned_ds\n",
    "\n",
    "mimic_preprocess(processed_datasets['mimic4'])['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58e2d2b-b3ac-4d4a-86c2-4cecc94f1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:09,022 || INFO || MedRL-CoT Preprocess - Found 19968 rows\n",
      "2025-06-03 18:54:09,026 || INFO || MedRL-CoT Preprocess - Fixed class naming for 21 rows (0.10516826923076923 %)\n",
      "2025-06-03 18:54:09,040 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 131 rows (0.6560496794871795 %)\n",
      "2025-06-03 18:54:09,041 || INFO || MedRL-CoT Preprocess - Dropped 240 invalid rows (1.201923076923077 %)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "      <th>case_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A sixteen year-old girl, presented to our Outp...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She was not able to maintain an erect posture ...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She would keep her head turned to the right an...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was a sideways bending of the back in th...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To counter the abnormal positioning of the bac...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19963</th>\n",
       "      <td>No significant muscle atrophy was present, and...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19964</th>\n",
       "      <td>The physical examination showed that both of h...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19965</th>\n",
       "      <td>Both knees were very soft and could touch the ...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19966</th>\n",
       "      <td>Upon palpation, the continuity of the quadrice...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19967</th>\n",
       "      <td>In addition, the patient could not complete kn...</td>\n",
       "      <td>symptoms_labs</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19728 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence          class  \\\n",
       "0      A sixteen year-old girl, presented to our Outp...  symptoms_labs   \n",
       "1      She was not able to maintain an erect posture ...  symptoms_labs   \n",
       "2      She would keep her head turned to the right an...  symptoms_labs   \n",
       "3      There was a sideways bending of the back in th...  symptoms_labs   \n",
       "4      To counter the abnormal positioning of the bac...  symptoms_labs   \n",
       "...                                                  ...            ...   \n",
       "19963  No significant muscle atrophy was present, and...  symptoms_labs   \n",
       "19964  The physical examination showed that both of h...  symptoms_labs   \n",
       "19965  Both knees were very soft and could touch the ...  symptoms_labs   \n",
       "19966  Upon palpation, the continuity of the quadrice...  symptoms_labs   \n",
       "19967  In addition, the patient could not complete kn...  symptoms_labs   \n",
       "\n",
       "       case_id  \n",
       "0           -1  \n",
       "1           -1  \n",
       "2           -1  \n",
       "3           -1  \n",
       "4           -1  \n",
       "...        ...  \n",
       "19963      912  \n",
       "19964      915  \n",
       "19965      915  \n",
       "19966      915  \n",
       "19967      915  \n",
       "\n",
       "[19728 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aug_preprocess(dataset):\n",
    "    cleaned_ds = dataset.copy()\n",
    "\n",
    "    # For loggin purposes\n",
    "    N = cleaned_ds.shape[0]\n",
    "    logger.info(f\"Found {N} rows\")\n",
    "    num_renames = cleaned_ds[cleaned_ds['class'].isin(['symptoms_lbs', 'symptoms_lads'])].shape[0]\n",
    "    logger.info(f\"Fixed class naming for {num_renames} rows ({(num_renames / N)*100} %)\")\n",
    "    \n",
    "    # Fix naming of some classes\n",
    "    cleaned_ds['class'] = cleaned_ds['class'].replace({'symptoms_lbs': 'symptoms_labs', 'symptoms_lads': 'symptoms_labs'})\n",
    "    \n",
    "    pos_invalids = cleaned_ds[~cleaned_ds['class'].isin(classes)]\n",
    "    swapped_values = pos_invalids[pos_invalids['sentence'].isin(classes)]    # Rows with swapped values\n",
    "    invalid_classes = pos_invalids[pos_invalids['class'].str.lower().isin(['', '0', '__', 'None', '[]', 'False', 'The', 'No'])]  # collect empty sentence and classes (Note that doing it here will catch the invalid swapped sentences as well)\n",
    "    ignore_classes = pos_invalids[pos_invalids['sentence'].str.contains('not a sentence')]\n",
    "    invalid_sentences = pos_invalids[pos_invalids['sentence'].str.lower().isin(['', '__', 'None', '[]', 'False', '()'])]\n",
    "\n",
    "    invalids = pos_invalids.loc[invalid_classes.index.union(invalid_sentences.index.union(ignore_classes.index))]\n",
    "    # invalids = pos_invalids.loc[invalid_classes.index.union(invalid_sentences.index)]\n",
    "    # invalids = pos_invalids[pos_invalids ['class'].isin(['', '0', '[]', 'False'])]   # Clearly invalids, temp drop to remove from our ceaned list\n",
    "    nonstd_classes = pos_invalids.drop(index=swapped_values.index.union(invalids.index))   # Get list of non-standard class rows\n",
    "    # print(ignore_classes.index[0] in list(nonstd_classes.index))\n",
    "    # print(ignore_classes.index[0] in list(invalids.index))\n",
    "    # print(nonstd_classes[nonstd_classes['sentence'].str.contains('not a sentence')])\n",
    "\n",
    "    # Get list of classes that can be classified as \"other\" with enough occurence (non-outliery)\n",
    "    # value_cnts = nonstd_classes['class'].value_counts()\n",
    "    # other_classes = value_cnts[value_cnts >= 5].index.tolist()\n",
    "    # other_class_rows = nonstd_classes[nonstd_classes['class'].isin(other_classes)]\n",
    "    # other_class_rows = other_class_rows[~other_class_rows['sentence'].str.contains('thought_process')]\n",
    "    # other_class_rows = other_class_rows[~other_class_rows['sentence'].str.contains('symptoms_labs')]\n",
    "    # other_class_indices = other_class_rows[~other_class_rows['sentence'].str.contains('diagnosis')].index  # Note we get rid of this because bad classification when it should've been \"diagnosis\", also removes \"diagnosis: \" sentences\n",
    "    # print(other_class_indices)\n",
    "    # print(value_cnts[value_cnts >= 5])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'symptoms_lbs'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'A'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'classification'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'No'])\n",
    "    # display(cleaned_ds[cleaned_ds['class'] == 'The'])\n",
    "\n",
    "    swapped_indices = swapped_values.index\n",
    "    cleaned_ds.loc[swapped_indices, ['sentence', 'class']] = cleaned_ds.loc[swapped_indices, ['class', 'sentence']].values # swap the values in indices where it's swapped\n",
    "    # cleaned_ds.loc[other_class_indices, 'class'] = 'other'\n",
    "    # cleaned_ds['class'] = cleaned_ds['class'].apply(lambda x: 'other' if x in other_classes else x)  # relabel non-standards to 'other'\n",
    "    drop_indices = cleaned_ds[~cleaned_ds['class'].isin(classes)].index\n",
    "    # print(ignore_classes.index in list(drop_indices))\n",
    "    # print(ignore_classes.index)\n",
    "    # print(list(drop_indices))\n",
    "    \n",
    "    cleaned_ds = cleaned_ds.drop(index=drop_indices) # drop all others that aren't in our list of classes + 'other' (basically all invalids)\n",
    "\n",
    "    # Summary\n",
    "    num_reclass = cleaned_ds[cleaned_ds['class'] == 'other'].shape[0]\n",
    "    # logger.info(f\"Re-classified {len(other_classes)} classes as 'other', or {num_reclass} rows ({(num_reclass / N)*100} %)\")\n",
    "    logger.info(f'Swapped class and sentence values of {swapped_indices.shape[0]} rows ({(swapped_indices.shape[0] / N)*100} %)')\n",
    "    logger.info(f'Dropped {drop_indices.shape[0]} invalid rows ({(drop_indices.shape[0] / N)*100} %)')\n",
    "\n",
    "    # change case_id into int\n",
    "    cleaned_ds['case_id'] = cleaned_ds['case_id'].astype(int)\n",
    "    # print(cleaned_ds[cleaned_ds['sentence'].str.contains('not a sentence')])\n",
    "    \n",
    "    return cleaned_ds\n",
    "\n",
    "aug_preprocess(processed_datasets['aug_med_notes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5643b621-e1f5-48c4-b7b6-69a0c4569c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "proc_funcs = {'mimic4': mimic_preprocess, 'aug_med_notes': aug_preprocess}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc24bb2-7506-4e5a-9115-a2951271fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:09,130 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-03 18:54:09,131 || INFO || MedRL-CoT Preprocess - Cleaning up aug_med_notes dataset\n",
      "2025-06-03 18:54:09,132 || INFO || MedRL-CoT Preprocess - Found 19968 rows\n",
      "2025-06-03 18:54:09,135 || INFO || MedRL-CoT Preprocess - Fixed class naming for 21 rows (0.10516826923076923 %)\n",
      "2025-06-03 18:54:09,148 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 131 rows (0.6560496794871795 %)\n",
      "2025-06-03 18:54:09,149 || INFO || MedRL-CoT Preprocess - Dropped 240 invalid rows (1.201923076923077 %)\n",
      "2025-06-03 18:54:09,151 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-03 18:54:09,152 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-03 18:54:09,153 || INFO || MedRL-CoT Preprocess - Cleaning up mimic4 dataset\n",
      "2025-06-03 18:54:09,155 || INFO || MedRL-CoT Preprocess - Found 29654 rows\n",
      "2025-06-03 18:54:09,158 || INFO || MedRL-CoT Preprocess - Fixed class naming for 282 rows (0.9509678289606799 %)\n",
      "2025-06-03 18:54:09,177 || INFO || MedRL-CoT Preprocess - Re-classified 18 classes as 'other', or 1073 rows (3.618398866931948 %)\n",
      "2025-06-03 18:54:09,179 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 571 rows (1.925541242328185 %)\n",
      "2025-06-03 18:54:09,179 || INFO || MedRL-CoT Preprocess - Dropped 169 invalid rows (0.5699062521076415 %)\n",
      "2025-06-03 18:54:09,183 || INFO || MedRL-CoT Preprocess - ==================================================\n"
     ]
    }
   ],
   "source": [
    "preprocessed_datasets = dict()\n",
    "for key, item in processed_datasets.items():\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"Cleaning up {key} dataset\")\n",
    "    # processed_datasets[key] = mimic_preprocess(processed_datasets[key])\n",
    "    # mimic_preprocess(processed_datasets[key])\n",
    "    preprocessed_datasets[key] = proc_funcs[key](processed_datasets[key])\n",
    "    logger.info(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970f8205-b8fb-4fd7-8c7a-1bb540cd4085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aug_med_notes', 'mimic4'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4410b801-5e22-47e8-8a7b-89c99c484140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "        15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "        41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
       "        54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,\n",
       "        67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
       "        80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
       "        93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "       106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "       119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
       "       132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144,\n",
       "       145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
       "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
       "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
       "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "       197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
       "       223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
       "       236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
       "       249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261,\n",
       "       262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
       "       275, 276, 277, 278, 279, 280, 281, 282, 283])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_datasets['mimic4']['case_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073ba94c-9033-41e0-977e-3663c6e95118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def join_sentence_class(group):\n",
    "#     return ' '.join(f\"{row['sentence']} <{row['class']}>\" for _, row in group.iterrows())\n",
    "    \n",
    "# # preprocessed_datasets['mimic4'].groupby('case_id').apply(join_sentence_class).reset_index(name='full_class_case').sort_values('case_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81b6f92f-91dc-48cd-bbdc-03b29b06270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_datasets = dict()\n",
    "# for key, dataset in preprocessed_datasets.items():\n",
    "#     cases_datasets[key] = dataset.groupby('case_id').apply(join_sentence_class).reset_index(name='full_class_case').sort_values('case_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "091b4a97-8f38-4cdc-855f-74fa96912fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_datasets['aug_med_notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af634f5-3278-41c4-b4f2-ad4689448f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_datasets['aug_med_notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2daf529-4d0a-49f9-87e9-4baa1d6e738a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessed_datasets['aug_med_notes'][preprocessed_datasets['aug_med_notes']['class'] == 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a78ec6d9-3ab1-4522-9810-d475db8e3f17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:09,530 || INFO || Logger - Setup for MedRL-CoT's log done. This is the beginning of the log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new log file logs/medrlcot122.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:09,732 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-03 18:54:09,734 || INFO || MedRL-CoT Preprocess - Cleaning up aug_med_notes dataset\n",
      "2025-06-03 18:54:09,736 || INFO || MedRL-CoT Preprocess - Found 19968 rows\n",
      "2025-06-03 18:54:09,739 || INFO || MedRL-CoT Preprocess - Fixed class naming for 21 rows (0.10516826923076923 %)\n",
      "2025-06-03 18:54:09,767 || INFO || MedRL-CoT Preprocess - Re-classified 5 classes as 'other', or 76 rows (0.38060897435897434 %)\n",
      "2025-06-03 18:54:09,768 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 131 rows (0.6560496794871795 %)\n",
      "2025-06-03 18:54:09,769 || INFO || MedRL-CoT Preprocess - Dropped 224 invalid rows (1.1217948717948718 %)\n",
      "2025-06-03 18:54:09,770 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-03 18:54:09,771 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "2025-06-03 18:54:09,772 || INFO || MedRL-CoT Preprocess - Cleaning up mimic4 dataset\n",
      "2025-06-03 18:54:09,773 || INFO || MedRL-CoT Preprocess - Found 29654 rows\n",
      "2025-06-03 18:54:09,776 || INFO || MedRL-CoT Preprocess - Fixed class naming for 282 rows (0.9509678289606799 %)\n",
      "2025-06-03 18:54:09,798 || INFO || MedRL-CoT Preprocess - Swapped class and sentence values of 571 rows (1.925541242328185 %)\n",
      "2025-06-03 18:54:09,799 || INFO || MedRL-CoT Preprocess - Dropped 1461 invalid rows (4.9268226883388415 %)\n",
      "2025-06-03 18:54:09,800 || INFO || MedRL-CoT Preprocess - ==================================================\n",
      "/tmp/ipykernel_129087/1970724310.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cases_datasets[key] = dataset.groupby('case_id').apply(mp.xy_split_processing_sft).reset_index().sort_values('case_id')\n",
      "/tmp/ipykernel_129087/1970724310.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cases_datasets[key] = dataset.groupby('case_id').apply(mp.xy_split_processing_sft).reset_index().sort_values('case_id')\n",
      "2025-06-03 18:54:11,805 || INFO || DataManager - Loading datasets: ['aug_med_notes', 'mimic4']\n",
      "2025-06-03 18:54:11,812 || INFO || DataManager - AGBonnet/augmented-clinical-notes dataset already exists in disk. If the dataset is giving errors or you'd like a fresh install, delete the /mnt/e/UCSD/ECE285DGM/ECE285DGMProject/data/aug_med_notes/train directory.\n",
      "2025-06-03 18:54:11,813 || INFO || DataManager - Loading saved hugginface AGBonnet/augmented-clinical-notes dataset.\n",
      "2025-06-03 18:54:11,852 || INFO || DataManager - Successfully loaded AGBonnet/augmented-clinical-notes as key aug_med_notes\n",
      "2025-06-03 18:54:11,857 || INFO || DataManager - discharge.csv.gz dataset already exists in disk as huggin_face dataset. If the dataset is giving errors or you'd like a fresh install, delete the /mnt/e/UCSD/ECE285DGM/ECE285DGMProject/data/mimic4/hf directory.\n",
      "2025-06-03 18:54:11,858 || INFO || DataManager - Loading saved hugginface discharge.csv.gz dataset.\n",
      "2025-06-03 18:54:12,154 || INFO || DataManager - Successfully loaded hugging_face of discharge.csv.gz as key mimic4\n",
      "2025-06-03 18:54:12,155 || INFO || DataManager - Successfully loaded 2 datasets: ['aug_med_notes', 'mimic4']\n"
     ]
    }
   ],
   "source": [
    "def xy_split_processing_sft(group, x_func=None, y_func=None):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for _, row in group.iterrows():\n",
    "        if row['class'] == 'symptoms_labs' or row['class'] == 'other':\n",
    "            X.append(row)\n",
    "        else:\n",
    "            Y.append(row)\n",
    "\n",
    "    # X_case = ' '.join(f\"{row['sentence']} <{row['class']}>\" for _, row in group.iterrows()) # Input with all\n",
    "    X_case = x_func(X) if x_func else ' '.join([str(row['sentence']) for row in X])\n",
    "    Y_case = ' '.join([f\"{row['sentence']} <{row['class']}> \" for row in Y])    # Output with only thought_process and diagnosis\n",
    "    \n",
    "    return pd.Series({'X': X_case, 'Y': Y_case})\n",
    "\n",
    "import medrlcot.preprocessing as mp\n",
    "preprocessed_datasets = mp.preprocess_datasets()\n",
    "\n",
    "# Combine cases into one for cases as example for SFT\n",
    "cases_datasets = dict()\n",
    "for key, dataset in preprocessed_datasets.items():\n",
    "    cases_datasets[key] = dataset.groupby('case_id').apply(mp.xy_split_processing_sft).reset_index().sort_values('case_id')\n",
    "    cases_datasets[key]['wc_x'] = cases_datasets[key]['X'].apply(lambda x: len(x.split())) \n",
    "    cases_datasets[key]['wc_y'] = cases_datasets[key]['Y'].apply(lambda x: len(x.split())) \n",
    "    cases_datasets[key] = cases_datasets[key][(cases_datasets[key]['wc_x'] <= 800) & (cases_datasets[key]['wc_y'] <= 900)].reset_index(drop=True)\n",
    "\n",
    "import os\n",
    "from medrlcot import data_manager\n",
    "import medrlcot.config.env as mce\n",
    "model_cfg_path = os.path.join(os.getcwd(), \"medrlcot/config/.env\")\n",
    "medrlcot_config = mce.MedRL_CoT(model_cfg_path)\n",
    "raw_datasets = data_manager.load_datasets(medrlcot_config.datasets, data_dir=medrlcot_config.data_dir)  # Load raw dataset (original cases) for RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d02ed4d7-95df-49bd-9597-b9e44a139564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>wc_x</th>\n",
       "      <th>wc_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Her past medical history included mild Multipl...</td>\n",
       "      <td>A 34 year old Persian woman, gravida 1, para 0...</td>\n",
       "      <td>147</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>A 60-year-old female who was previously health...</td>\n",
       "      <td>She was treated with thrombolysis and endovasc...</td>\n",
       "      <td>168</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td></td>\n",
       "      <td>classification &lt;diagnosis&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>The chief complains included bilateral hips an...</td>\n",
       "      <td>Staged bilateral total hip arthroplasties were...</td>\n",
       "      <td>362</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104</td>\n",
       "      <td>The patient is a male in his 60s with a past m...</td>\n",
       "      <td>The initial workup was unrevealing for an etio...</td>\n",
       "      <td>236</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>93</td>\n",
       "      <td>The patient was a 60-year-old man who referred...</td>\n",
       "      <td>A 45 mm × 37 mm pseudoaneurysm in lateral side...</td>\n",
       "      <td>317</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>94</td>\n",
       "      <td>A 38 year old Vietnamese man was admitted with...</td>\n",
       "      <td>The working diagnosis was a collection seconda...</td>\n",
       "      <td>135</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>96</td>\n",
       "      <td>A 30-year-old woman was admitted to our instit...</td>\n",
       "      <td>She had undergone endovascular trapping of the...</td>\n",
       "      <td>148</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>97</td>\n",
       "      <td>A 51-year-old hypertensive Pakistani male pati...</td>\n",
       "      <td>The patient underwent general anesthesia for t...</td>\n",
       "      <td>275</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>98</td>\n",
       "      <td>An 18-year-old male patient (height 140 cm, we...</td>\n",
       "      <td>The patient was diagnosed with spinal muscular...</td>\n",
       "      <td>139</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>737 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    case_id                                                  X  \\\n",
       "0       100  Her past medical history included mild Multipl...   \n",
       "1       101  A 60-year-old female who was previously health...   \n",
       "2       102                                                      \n",
       "3       103  The chief complains included bilateral hips an...   \n",
       "4       104  The patient is a male in his 60s with a past m...   \n",
       "..      ...                                                ...   \n",
       "732      93  The patient was a 60-year-old man who referred...   \n",
       "733      94  A 38 year old Vietnamese man was admitted with...   \n",
       "734      96  A 30-year-old woman was admitted to our instit...   \n",
       "735      97  A 51-year-old hypertensive Pakistani male pati...   \n",
       "736      98  An 18-year-old male patient (height 140 cm, we...   \n",
       "\n",
       "                                                     Y  wc_x  wc_y  \n",
       "0    A 34 year old Persian woman, gravida 1, para 0...   147   304  \n",
       "1    She was treated with thrombolysis and endovasc...   168   255  \n",
       "2                          classification <diagnosis>      0     2  \n",
       "3    Staged bilateral total hip arthroplasties were...   362   314  \n",
       "4    The initial workup was unrevealing for an etio...   236   149  \n",
       "..                                                 ...   ...   ...  \n",
       "732  A 45 mm × 37 mm pseudoaneurysm in lateral side...   317   185  \n",
       "733  The working diagnosis was a collection seconda...   135   415  \n",
       "734  She had undergone endovascular trapping of the...   148   587  \n",
       "735  The patient underwent general anesthesia for t...   275   183  \n",
       "736  The patient was diagnosed with spinal muscular...   139    77  \n",
       "\n",
       "[737 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_datasets['aug_med_notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bce3bcdf-2c5e-4959-b97b-9c617082eeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "symptoms_labs      9977\n",
       "thought_process    5318\n",
       "diagnosis          4373\n",
       "other                76\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_datasets['aug_med_notes']['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d265d0e4-663a-49c2-b5ca-050e0567eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_datasets = dict()\n",
    "# for key, dataset in cases_datasets.items():\n",
    "#     dataset['wc_x'] = dataset['X'].apply(lambda x: len(x.split())) \n",
    "#     dataset['wc_y'] = dataset['Y'].apply(lambda x: len(x.split())) \n",
    "#     filtered_datasets[key] = dataset[(dataset['wc_x'] <= 800) & (dataset['wc_y'] <= 900)].reset_index(drop=True)\n",
    "\n",
    "# print(cases_datasets['aug_med_notes'].shape, filtered_datasets['aug_med_notes'].shape)\n",
    "# print(cases_datasets['mimic4'].shape, filtered_datasets['mimic4'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d61fa9a-274c-4cd9-b23b-975b641570ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('mimic4', (32, case_id                                                  135\n",
      "X          History of Present Illness: Mr. ___ is a ___ y...\n",
      "Y          1. <diagnosis>  2. <diagnosis>  3. <diagnosis>...\n",
      "wc_x                                                     791\n",
      "wc_y                                                     486\n",
      "Name: 32, dtype: object)), 791) (('mimic4', (48, case_id                                                  152\n",
      "X          Today, patient has a new cough with productive...\n",
      "Y          She received 1L NS, 5 mg of morphine, 25 mg of...\n",
      "wc_x                                                     783\n",
      "wc_y                                                     362\n",
      "Name: 48, dtype: object)), 783)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = (None, 0)\n",
    "next_max = (None, -1)\n",
    "num_exceed = 0\n",
    "for key, dataset in cases_datasets.items():\n",
    "    for row in dataset.iterrows():\n",
    "        cnt = ((key, row), len(row[1]['X'].split()))\n",
    "        if cnt[1] > 750:\n",
    "            num_exceed += 1\n",
    "        if cnt[1] > max_words[1]:\n",
    "            next_max = max_words\n",
    "            max_words = cnt\n",
    "        else:\n",
    "            next_max = max(cnt, next_max, key=lambda x: x[1])\n",
    "print(max_words, next_max)\n",
    "num_exceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ff63a06-c251-403a-b97f-3eec8fd85fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:12,775 || INFO || DataManager - Using train-val split: [0.75, 0.25]\n",
      "2025-06-03 18:54:12,777 || INFO || DataManager - Split-Shuffle with seed 0\n",
      "2025-06-03 18:54:12,778 || INFO || DataManager - Splitting aug_med_notes dataset.\n",
      "2025-06-03 18:54:12,782 || INFO || DataManager - Split into 552 train rows and 185 val rows\n",
      "2025-06-03 18:54:12,783 || INFO || DataManager - Splitting mimic4 dataset.\n",
      "2025-06-03 18:54:12,785 || INFO || DataManager - Split into 175 train rows and 59 val rows\n",
      "2025-06-03 18:54:12,786 || INFO || DataManager - Creating a single joint dataset of dict_keys(['aug_med_notes', 'mimic4']) dataset splits\n",
      "2025-06-03 18:54:12,789 || INFO || DataManager - Joined 727 train rows\n",
      "2025-06-03 18:54:12,790 || INFO || DataManager - Shuffling train's rows\n",
      "2025-06-03 18:54:12,792 || INFO || DataManager - Joined 244 val rows\n",
      "2025-06-03 18:54:12,792 || INFO || DataManager - Shuffling val's rows\n",
      "2025-06-03 18:54:12,794 || INFO || DataManager - Returning shuffled train-val splits\n"
     ]
    }
   ],
   "source": [
    "# Create customd dataset obj\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "cases_data = data_manager.MedRL_CoT_Dataset(cases_datasets, seed=0, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6577f860-e2bb-45e2-b632-1226e0a05543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In December 2006, about 6 months earlier than ...</td>\n",
       "      <td>Pain of the first mentioned episode was subsid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The patient was suffering from occasional blee...</td>\n",
       "      <td>An 80-year-old male was referred to us with sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The culture result showed polymicrobial growth.</td>\n",
       "      <td>The patient was hospitalized for 2 months to r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 55 years old male patient received a locki...</td>\n",
       "      <td>The patient consented to implant removal, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prior to treatment with omalizumab and after c...</td>\n",
       "      <td>While mast cells represent the effector cell i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>History of Present Illness: Ms. ___ is a ___ w...</td>\n",
       "      <td>Of note, patient was recently admitted  &lt;thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>This 36-year-old man presented with a three-mo...</td>\n",
       "      <td>Sinus plain radiographs demonstrated ‘sinusiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>A 60-year-old male who had a history of liver ...</td>\n",
       "      <td>After admission, the patient maintained with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>She also had diabetes mellitus, hypertension, ...</td>\n",
       "      <td>An 80-year-old woman with a history of collaps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>This case presents a 38-year-old Asian G2P1001...</td>\n",
       "      <td>The patient declined the option of removing th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>727 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     X  \\\n",
       "0    In December 2006, about 6 months earlier than ...   \n",
       "1    The patient was suffering from occasional blee...   \n",
       "2      The culture result showed polymicrobial growth.   \n",
       "3    The 55 years old male patient received a locki...   \n",
       "4    Prior to treatment with omalizumab and after c...   \n",
       "..                                                 ...   \n",
       "722  History of Present Illness: Ms. ___ is a ___ w...   \n",
       "723  This 36-year-old man presented with a three-mo...   \n",
       "724  A 60-year-old male who had a history of liver ...   \n",
       "725  She also had diabetes mellitus, hypertension, ...   \n",
       "726  This case presents a 38-year-old Asian G2P1001...   \n",
       "\n",
       "                                                     Y  \n",
       "0    Pain of the first mentioned episode was subsid...  \n",
       "1    An 80-year-old male was referred to us with sq...  \n",
       "2    The patient was hospitalized for 2 months to r...  \n",
       "3    The patient consented to implant removal, with...  \n",
       "4    While mast cells represent the effector cell i...  \n",
       "..                                                 ...  \n",
       "722  Of note, patient was recently admitted  <thoug...  \n",
       "723  Sinus plain radiographs demonstrated ‘sinusiti...  \n",
       "724  After admission, the patient maintained with a...  \n",
       "725  An 80-year-old woman with a history of collaps...  \n",
       "726  The patient declined the option of removing th...  \n",
       "\n",
       "[727 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ac83011-3165-4c21-81f8-3ae069bd813a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7717d35d7c40>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data.get_dataloader('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2524fc2-3421-43ef-a89a-4c2e3167d7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7717d35d5a80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_data.get_dataloader('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44c1b78f-0f78-4011-9270-b3e3ecfe14f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc857fc83a4b441db57aebb95c0ee6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model  = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f88ea78e-f58c-4e93-a078-dd644f267828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b18df7ed-69a4-4be2-95ea-3c03fa9ff047",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0d9e217-fc17-4cdd-a578-070737acec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds): # https://www.datacamp.com/tutorial/flan-t5-tutorial\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "420d2d61-1b6b-4328-9a70-9faa9544a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok_dataset = cases_data.get_torch_dataset('train')\n",
    "val_tok_dataset = cases_data.get_torch_dataset('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39ca4695-8d06-4388-bfa2-6e673f19c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_tok_dataset,\n",
    "    eval_dataset=val_tok_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9653c-0e0c-4e1b-90f6-f7a1d078478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Pikasannnnn/.pyenv/versions/anaconda3-2024.06-1/envs/medrlcot/lib/python3.10/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='2181' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  25/2181 18:41 < 29:11:24, 0.02 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1d4bb-c6d9-4a71-a35a-eaa11746b28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10379d3d-3348-4258-aa46-4c7113516f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
